{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53f8680",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Idefics3ForConditionalGeneration\n",
    "from fuzzywuzzy import fuzz\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import base64\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a4c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processor...\n",
      "Loading model...\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Path to the saved model\n",
    "model_path = \"./SmolDocling-256M-preview-30--NoLoc\"\n",
    "\n",
    "# Load model and processor\n",
    "print(\"Loading processor...\")\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = Idefics3ForConditionalGeneration.from_pretrained(model_path).to(DEVICE)\n",
    "model.eval()\n",
    "print(\"Model loaded.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d2611b",
   "metadata": {},
   "source": [
    "#### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba81713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./Data/test/24c6b7320051721a/1435963b-528c-4d02-9d7e-2f75da33d9d2.json'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getJsonFiles(DataFolder,Type):\n",
    "    \"\"\"   Get all JSON files from the specified folder and its subfolders.\"\"\"\n",
    "    Files = []\n",
    "    subFolder = os.path.join(DataFolder, Type)\n",
    "    for folder in os.listdir(subFolder):  \n",
    "        folder_path = os.path.join(subFolder, folder)\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith('.json') and not file.endswith('_processed.json'):\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                Files.append(file_path)\n",
    "    return Files\n",
    "\n",
    "def get_image_from_json(json_file_path):\n",
    "    \"\"\" Load an image from a JSON file containing base64 encoded image data.\"\"\"\n",
    "    with open(json_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    base64_image = data[\"pages\"][0][\"image\"][\"content\"]\n",
    "    image_data = base64.b64decode(base64_image)\n",
    "    image = Image.open(BytesIO(image_data))\n",
    "    return image.convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa82961",
   "metadata": {},
   "outputs": [],
   "source": [
    "Files = getJsonFiles(\"./Data\",'test')\n",
    "json_path = Files[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4462c890",
   "metadata": {},
   "source": [
    "#### Formating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff3527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def doctag2Json(doctag, labels):\n",
    "    \"\"\" Convert a doctag string to a JSON object with specified labels.\"\"\"\n",
    "    try : \n",
    "        result = {}\n",
    "        entities = doctag.strip().split(\"</text>\")\n",
    "        entities = [e.replace(\"<text>\", \"\").strip() for e in entities if e.strip()]\n",
    "        \n",
    "        cleaned_entities = []\n",
    "        for e in entities:\n",
    "            cleaned_entities.append(e.split(\":\", 1)[1].strip())\n",
    "\n",
    "        \n",
    "        for index, label in enumerate(labels):\n",
    "            result[label] = cleaned_entities[index]\n",
    "    except:\n",
    "        print(\"error\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "def extract_doctags(model_output):\n",
    "    # Find all <text>...</text> blocks\n",
    "    doctags = re.findall(r'<text>.*?</text>', model_output, re.DOTALL)\n",
    "    return \"\\n\".join(doctags)\n",
    "\n",
    "labels = [\n",
    "    \"Adresse-prescripteur\",\n",
    "    \"Date-de-la-prescription\",\n",
    "    \"Nom-du-medecin\",\n",
    "    \"Numero-ADELI\",\n",
    "    \"Numero-AM-Finess\",\n",
    "    \"Numero-RPPS\",\n",
    "    \"Signature\",\n",
    "    \"Texte-manuscrit\",\n",
    "    \"Texte-Signature\",\n",
    "    \"Texte-soin-ALD\",\n",
    "    \"Texte-soin-sans-ALD\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f03722",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = get_image_from_json(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a199012f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wassi/myvenv/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def benchmark_json(true_json, pred_json):\n",
    "    \"\"\" Compare two JSON objects and return hard and fuzzy match scores for each label.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for label in true_json:\n",
    "        true_value = (true_json.get(label) or \"\").strip()\n",
    "        pred_value = (pred_json.get(label) or \"\").strip()\n",
    "\n",
    "        if pred_value == \"\":\n",
    "            pred_value = \"None\"\n",
    "        if true_value == \"\":\n",
    "            true_value = \"None\"\n",
    "        \n",
    "        hard_match = int(true_value == pred_value)\n",
    "\n",
    "        fuzzy_match = fuzz.ratio(true_value, pred_value) / 100.0  \n",
    "        \n",
    "        results[label] = {\n",
    "            \"hard_match\": hard_match,\n",
    "            \"fuzzy_match\": round(fuzzy_match, 4)  \n",
    "        }\n",
    "\n",
    "    return results\n",
    "    \n",
    "def merge_benchmarks(all_step_jsons):\n",
    "    \"\"\" Merge multiple JSON benchmark results and calculate average scores for each label.\"\"\"\n",
    "    from collections import defaultdict\n",
    "\n",
    "    merged = defaultdict(lambda: {\"hard_match\": [], \"fuzzy_match\": []})\n",
    "\n",
    "    for step_json in all_step_jsons:\n",
    "        for label, scores in step_json.items():\n",
    "            merged[label][\"hard_match\"].append(scores.get(\"hard_match\", 0))\n",
    "            merged[label][\"fuzzy_match\"].append(scores.get(\"fuzzy_match\", 0.0))\n",
    "\n",
    "    averaged = {}\n",
    "    for label, scores in merged.items():\n",
    "        avg_hard = round(sum(scores[\"hard_match\"]) / len(scores[\"hard_match\"]), 4)\n",
    "        avg_fuzzy = round(sum(scores[\"fuzzy_match\"]) / len(scores[\"fuzzy_match\"]), 4)\n",
    "        averaged[label] = {\n",
    "            \"hard_match\": avg_hard,\n",
    "            \"fuzzy_match\": avg_fuzzy\n",
    "        }\n",
    "\n",
    "    return averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb3d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model):\n",
    "    \"\"\" Evaluate the model on the test dataset and return averaged benchmark results.\"\"\"\n",
    "\n",
    "    benchmark_jsons = []\n",
    "    for json_path in Files:\n",
    "        image = get_image_from_json(json_path)\n",
    "\n",
    "        # Construct prompt\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Convert this page to docling.\"},\n",
    "                    {\"type\": \"image\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        chat = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "        inputs = processor(text=chat, images=[[image]], return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "\n",
    "        # Generate response\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(**inputs, max_new_tokens=512)\n",
    "\n",
    "        # Decode result\n",
    "        output_text = processor.batch_decode(output_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        doctags = extract_doctags(output_text)\n",
    "        predicted_json = doctag2Json(doctags, labels)\n",
    "\n",
    "        json_processed_path = json_path.replace(\".json\", \"_processed.json\")\n",
    "\n",
    "        try:\n",
    "            with open(json_processed_path, 'r', encoding=\"utf-8\") as file:\n",
    "                true_json = json.load(file)\n",
    "                step_benchmark_json = benchmark_json(true_json,predicted_json)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading JSON from {json_path}: {e}\")\n",
    "        \n",
    "        benchmark_jsons.append(step_benchmark_json)\n",
    "\n",
    "    \n",
    "    averaged_json = merge_benchmarks(benchmark_jsons)\n",
    "\n",
    "    return averaged_json\n",
    "\n",
    "average_json = evaluate_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
